{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhV0oDB2X95I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import deque\n",
        "import os\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyZUp1VbX_DK",
        "outputId": "3e1678fb-e338-42c9-b3c7-0c1810bbe60b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oLeb5fhxxpINkFARaqWU3us4dqmA5JR7\n",
            "To: /content/malicious_hash.csv\n",
            "100%|██████████| 3.11k/3.11k [00:00<00:00, 8.81MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1XOkeFjqjBxI_1t7uGPh8S_-QZPN5disk\n",
            "From (redirected): https://drive.google.com/uc?id=1XOkeFjqjBxI_1t7uGPh8S_-QZPN5disk&confirm=t&uuid=3d5c9ea7-b9be-4704-aac2-9fb9f1457ca6\n",
            "To: /content/DG_out.csv\n",
            "100%|██████████| 4.16G/4.16G [01:28<00:00, 47.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "mal_file_id = \"1oLeb5fhxxpINkFARaqWU3us4dqmA5JR7\"\n",
        "mal_url = f\"https://drive.google.com/uc?id={mal_file_id}\"\n",
        "mal_file = gdown.download(mal_url, quiet=False)\n",
        "MALICIOUS_HASH_FILE = pd.read_csv(mal_file)\n",
        "MALICIOUS_HASH_FILE.head()\n",
        "\n",
        "\n",
        "dg_file_id = \"1XOkeFjqjBxI_1t7uGPh8S_-QZPN5disk\"\n",
        "dg_url = f\"https://drive.google.com/uc?id={dg_file_id}\"\n",
        "dg_file = gdown.download(dg_url, quiet=False)\n",
        "FULL_NODE_FILE = pd.read_csv(dg_file)\n",
        "FULL_NODE_FILE.head()\n",
        "\n",
        "\n",
        "txn_file_id = \"1QLleaYa0Z0wxzKWiUue94Q99ziPNsa9H\"\n",
        "txn_url = f\"https://drive.google.com/uc?id={txn_file_id}\"\n",
        "txn_file = gdown.download(txn_url, quiet=False)\n",
        "FULL_EDGE_FILE = pd.read_csv(txn_file)\n",
        "FULL_EDGE_FILE.head()\n",
        "\n",
        "# Target size for each BFS search\n",
        "NODES_PER_BFS = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZqYSUSJYCap"
      },
      "outputs": [],
      "source": [
        "# Load malicious hashes\n",
        "if not os.path.exists(MALICIOUS_HASH_FILE):\n",
        "    raise FileNotFoundError(f\"Could not find malicious hash file at: {MALICIOUS_HASH_FILE}\")\n",
        "malicious_txn = pd.read_csv(MALICIOUS_HASH_FILE)\n",
        "malicious_hashes_list = malicious_txn['hash'].tolist()\n",
        "print(f\"Loaded {len(malicious_hashes_list)} malicious seed hashes.\")\n",
        "\n",
        "# Load full node feature table\n",
        "print(f\"Loading full node file: {FULL_NODE_FILE}...\")\n",
        "if not os.path.exists(FULL_NODE_FILE):\n",
        "    raise FileNotFoundError(f\"Could not find full node file at: {FULL_NODE_FILE}\")\n",
        "nodes_df = pd.read_csv(FULL_NODE_FILE)\n",
        "\n",
        "# Load full edge list\n",
        "print(f\"Loading full edge file: {FULL_EDGE_FILE}...\")\n",
        "if not os.path.exists(FULL_EDGE_FILE):\n",
        "    raise FileNotFoundError(f\"Could not find full edge file at: {FULL_EDGE_FILE}\")\n",
        "column_names = [\"tx_hash_from\", \"tx_hash_to\", \"datetime\", \"amount_bitcoins\"]\n",
        "txn_df = pd.read_csv(FULL_EDGE_FILE,header= None, names=column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlxVnUXuYGRf"
      },
      "outputs": [],
      "source": [
        "# Build a directed graph from the edge list\n",
        "G = nx.from_pandas_edgelist(\n",
        "    txn_df,\n",
        "    source='tx_hash_from',\n",
        "    target='tx_hash_to',\n",
        "    create_using=nx.DiGraph()\n",
        ")\n",
        "print(f\"Graph built successfully.\")\n",
        "print(f\"- Total Nodes: {G.number_of_nodes():,}\")\n",
        "print(f\"- Total Edges: {G.number_of_edges():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEOilVEIYOB8"
      },
      "outputs": [],
      "source": [
        "# DEFINE BFS SAMPLER\n",
        "# ===================================================================\n",
        "\n",
        "def bfs_bidirectional(G, start_tx, target_n):\n",
        "    \"\"\"\n",
        "    Performs a Breadth-First Search starting from 'start_tx'.\n",
        "    It hops BOTH WAYS (successors and predecessors) to find\n",
        "    the complete local neighborhood.\n",
        "    Stops when 'target_n' unique nodes are found.\n",
        "    \"\"\"\n",
        "    visited = set([start_tx])\n",
        "    q = deque([start_tx])\n",
        "\n",
        "    # Check if start node is in graph\n",
        "    if not G.has_node(start_tx):\n",
        "        print(f\"  Warning: Start node {start_tx} not found in graph.\")\n",
        "        return set() # Return empty set\n",
        "\n",
        "    while q and len(visited) < target_n:\n",
        "        node = q.popleft()\n",
        "        # Get all nodes pointing TO this node (predecessors)\n",
        "        preds = G.predecessors(node)\n",
        "        # Get all nodes this node points TO (successors)\n",
        "        succs = G.successors(node)\n",
        "\n",
        "        # Combine them\n",
        "        import itertools\n",
        "        all_neighbors = itertools.chain(preds, succs)\n",
        "\n",
        "        for nb in all_neighbors:\n",
        "            if nb not in visited:\n",
        "                visited.add(nb)\n",
        "                q.append(nb)\n",
        "                if len(visited) >= target_n:\n",
        "                    break\n",
        "\n",
        "    return visited"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHCxdIeuYV9E"
      },
      "outputs": [],
      "source": [
        "all_subgraph_nodes = set()\n",
        "\n",
        "for i, start_hash in enumerate(malicious_hashes_list):\n",
        "    # Progress indicator\n",
        "    print(f\"Processing {i+1}/{len(malicious_hashes_list)}: {start_hash}...\", end=\"\\r\")\n",
        "\n",
        "    # Optimization: If this malicious node was already picked up\n",
        "    # by a previous search, we don't need to search it again.\n",
        "    if start_hash in all_subgraph_nodes:\n",
        "        continue\n",
        "\n",
        "    # Run the bidirectional BFS\n",
        "    node_set = bfs_bidirectional(G, start_hash, target_n=NODES_PER_BFS)\n",
        "\n",
        "    # Add new unique nodes to our master set\n",
        "    all_subgraph_nodes.update(node_set)\n",
        "\n",
        "print(f\"\\nSampling complete.\")\n",
        "print(f\"Total unique nodes selected for final dataset: {len(all_subgraph_nodes):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q64DbZ23Yb03"
      },
      "outputs": [],
      "source": [
        "# 1. Filter Node Features\n",
        "print(\"Filtering node feature table...\")\n",
        "nodes_filtered = nodes_df[\n",
        "    nodes_df[\"tx_hash\"].isin(all_subgraph_nodes)\n",
        "].copy()\n",
        "print(f\"- Filtered node table size: {len(nodes_filtered):,} rows\")\n",
        "\n",
        "# 2. Filter Edges\n",
        "# We only keep edges that connect two nodes within our selected set.\n",
        "print(\"Filtering transaction edge table...\")\n",
        "txn_filtered = txn_df[\n",
        "    txn_df[\"tx_hash_from\"].isin(all_subgraph_nodes) &\n",
        "    txn_df[\"tx_hash_to\"].isin(all_subgraph_nodes)\n",
        "].copy()\n",
        "print(f\"- Filtered edge table size: {len(txn_filtered):,} rows\")\n",
        "\n",
        "# 3. Save to CSV\n",
        "output_node_file = f\"nodes_filtered_{len(all_subgraph_nodes)//1000}k.csv\"\n",
        "output_edge_file = f\"txn_filtered_{len(all_subgraph_nodes)//1000}k.csv\"\n",
        "\n",
        "print(f\"Saving {output_node_file}...\")\n",
        "nodes_filtered.to_csv(output_node_file, index=False)\n",
        "\n",
        "print(f\"Saving {output_edge_file}...\")\n",
        "txn_filtered.to_csv(output_edge_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrDLiQePYdXE"
      },
      "outputs": [],
      "source": [
        "print(f\"1. Node Features: {output_node_file}\")\n",
        "print(f\"2. Edge List:     {output_edge_file}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
